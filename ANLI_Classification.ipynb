{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8113ca-cf81-436d-9223-6cd769ff9d8d",
   "metadata": {},
   "source": [
    "# HIGH-PERFORMANCE ANLI FINETUNING (GPU ENABLED)\n",
    "**Goals**\n",
    "- End-to-end ML pipeline: EDA → Preprocessing → Training → Evaluation → Save & Deploy.\n",
    "- Save metrics, plots, confusion matrix, model, and tokenizer.\n",
    "- Provide GitHub & Docker deployment instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7ebca-2c5e-4e79-9112-7bceb9dca96f",
   "metadata": {},
   "source": [
    "## Summary & How to use this notebook\n",
    "\n",
    "This notebook fine-tunes a transformer on the ANLI dataset (optionally using all rounds).\n",
    "It is structured to be reproducible and GPU-friendly:\n",
    "1. Configuration & reproducibility\n",
    "2. Load data & EDA (class distribution, basic stats)\n",
    "3. Class balancing (upsampling minority classes) — optional but used here\n",
    "4. Tokenization & preprocessing\n",
    "5. Model + training setup (Hugging Face Trainer)\n",
    "6. Evaluation, confusion matrix, training curves\n",
    "7. Save artifacts and reproducibility metadata\n",
    "\n",
    "**Notes**\n",
    "- Main metric: Macro F1 (robust to class imbalance).\n",
    "- Uses `DataCollatorWithPadding` for efficient dynamic padding.\n",
    "- Adjust `USE_ALL_ROUNDS`, `MODEL_NAME`, and hyperparameters in the config cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd56b9-8a4f-466b-acef-7da96f03d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback,\n",
    "    set_seed\n",
    "\n",
    "# Output Directories\n",
    "OUT_DIR = Path(\"./anli_best_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=4, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cee28-2a85-41be-9fa2-0dc28366dbd7",
   "metadata": {},
   "source": [
    "# 0. CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0b3c-90c1-43ca-8832-9cb93f964e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALL_ROUNDS = True        # If True, use train_r1+r2+r3; else use only r2 (smaller)\n",
    "MODEL_NAME = \"roberta-large\" # e.g., \"roberta-large\" or \"microsoft/deberta-v3-large\"\n",
    "MAX_LENGTH = 256             # Tokenization max length\n",
    "BATCH_SIZE = 8               # per-device; effective batch = BATCH_SIZE * grad_acc * n_devices\n",
    "GRAD_ACC = 4                 # gradient accumulation to simulate larger batch size\n",
    "LR = 1e-5                    # base learning rate (typical for fine-tuning large models)\n",
    "NUM_EPOCHS = 3               # short fine-tune; increase if underfitting\n",
    "SEED = 42                    # reproducibility\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983d5de-3538-4e35-8a79-558a323e21bf",
   "metadata": {},
   "source": [
    "# 1. REPRODUCIBILITY\n",
    "\n",
    "Set seeds for Python, NumPy, Torch and (if available) CUDA.\n",
    "This helps results be reproducible across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bbbb7-74ad-4a5b-b19c-6407f0c82bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31226-4ab9-4168-ae1b-654883d2d047",
   "metadata": {},
   "source": [
    "# 2. LOAD DATA & SIMPLE EDA\n",
    "\n",
    "We load ANLI via `datasets`. Optionally concatenate all rounds (R1+R2+R3).\n",
    "We then print dataset sizes and class distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c2822-6ca5-4a54-9f8f-3686e2f293ff",
   "metadata": {},
   "source": [
    "### 2.1 Why care about class distribution?\n",
    "ANLI is adversarial and can be imbalanced. We use Macro-F1 as the primary metric because it averages per-class F1 and is robust to class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e685aa4-9411-4065-b2e2-2be85a753a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/anli\")\n",
    "\n",
    "if USE_ALL_ROUNDS:\n",
    "    train = concatenate_datasets([dataset[\"train_r1\"], dataset[\"train_r2\"], dataset[\"train_r3\"]])\n",
    "    dev   = concatenate_datasets([dataset[\"dev_r1\"],   dataset[\"dev_r2\"],   dataset[\"dev_r3\"]])\n",
    "    test  = concatenate_datasets([dataset[\"test_r1\"],  dataset[\"test_r2\"],  dataset[\"test_r3\"]])\n",
    "else:\n",
    "    train = dataset[\"train_r2\"]\n",
    "    dev   = dataset[\"dev_r2\"]\n",
    "    test  = dataset[\"test_r2\"]\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5029dab-64e5-4db2-bfeb-bd706a0486e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_stats(ds, name):\n",
    "    labels = np.array(ds[\"label\"])\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"{name} size: {len(ds)}\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  {label_map[u]:12s}: {c} ({c/len(ds):.2%})\")\n",
    "\n",
    "dataset_stats(train, \"Train\")\n",
    "dataset_stats(dev, \"Dev\")\n",
    "dataset_stats(test, \"Test\")\n",
    "\n",
    "eda_summary = {\n",
    "    \"train_size\": len(train),\n",
    "    \"dev_size\": len(dev),\n",
    "    \"test_size\": len(test),\n",
    "    \"train_class_counts\": dict(zip(*np.unique(np.array(train[\"label\"]), return_counts=True))),\n",
    "}\n",
    "save_json(eda_summary, OUT_DIR / \"eda_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38626cd1-15cf-4d7d-a3d0-b49698444acb",
   "metadata": {},
   "source": [
    "# 3. TOKENIZER & PREPROCESSING\n",
    "\n",
    "We use the model's tokenizer (RoBERTa or DeBERTa) and tokenize premise + hypothesis pair.\n",
    "We avoid padding here (use DataCollatorWithPadding at runtime for efficiency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5424fa-1046-4b48-8818-c897b88e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"premise\"], batch[\"hypothesis\"],\n",
    "                     truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "train_tok = train.map(preprocess, batched=True, remove_columns=train.column_names)\n",
    "dev_tok   = dev.map(preprocess,   batched=True, remove_columns=dev.column_names)\n",
    "test_tok  = test.map(preprocess,  batched=True, remove_columns=test.column_names)\n",
    "\n",
    "train_tok = train_tok.add_column(\"labels\", train[\"label\"])\n",
    "dev_tok   = dev_tok.add_column(\"labels\", dev[\"label\"])\n",
    "test_tok  = test_tok.add_column(\"labels\", test[\"label\"])\n",
    "\n",
    "train_tok.set_format(type=\"torch\")\n",
    "dev_tok.set_format(type=\"torch\")\n",
    "test_tok.set_format(type=\"torch\")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0dd46-70dc-4a74-84bd-c8f489759f4d",
   "metadata": {},
   "source": [
    "# 4. MODEL\n",
    "\n",
    "Load a sequence classification model with `num_labels=3`.\n",
    "Move model to device (Trainer will manage device during training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52291-1012-433a-a6a3-80f744c1c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93add1a2-0a09-4f3b-8101-230ff9fa913c",
   "metadata": {},
   "source": [
    "# 5. METRICS & EVAL FUNCTIONS\n",
    "\n",
    "We compute accuracy, per-class F1, and macro F1 (primary metric). The `compute_metrics` function receives logits and labels from Trainer predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b41e8-fcfb-4e23-8d1f-88c858857d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, labels=[0,1,2], average=None, zero_division=0)\n",
    "    macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"f1_entailment\": float(f1[0]),\n",
    "        \"f1_neutral\": float(f1[1]),\n",
    "        \"f1_contradiction\": float(f1[2]),\n",
    "    }\n",
    "\n",
    "def get_history(trainer):\n",
    "    history = trainer.state.log_history\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05fd64-639f-41d7-9439-0fdc735a9105",
   "metadata": {},
   "source": [
    "# 6. TRAINING ARGUMENTS & CALLBACKS\n",
    "\n",
    "Key choices explained:\n",
    "- `fp16`: Use mixed-precision if GPU available for faster training & lower memory.\n",
    "- `gradient_accumulation_steps`: simulate larger batch sizes.\n",
    "- `metric_for_best_model=\"macro_f1\"`: primary metric.\n",
    "- `save_total_limit=2`: keep disk usage small.\n",
    "- `eval_steps` & `save_steps` control evaluation frequency (500 is example; adapt for dataset size & GPU time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc5c93-3715-4763-a2e6-09a4b79fcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True if DEVICE == \"cuda\" else False,\n",
    "    report_to=\"none\")\n",
    "\n",
    "earlystop = EarlyStoppingCallback(early_stopping_patience=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099fce-ee2e-4561-9c8e-dea4c815e481",
   "metadata": {},
   "source": [
    "# 7. TRAINER SETUP\n",
    "We pass model, datasets, tokenizer, collator, compute_metrics and callbacks to Trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106c242-b550-4e03-b356-f7549ffdb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[earlystop],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cf669-ebc8-463a-8ba9-e4c744815133",
   "metadata": {},
   "source": [
    "# 8. TRAIN THE MODEL\n",
    "\n",
    "We record start/end timestamps and save the Trainer history for reproducibility and plotting.\n",
    "If GPU memory is a concern, reduce BATCH_SIZE or use gradient_accumulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6eeaf9-de9e-464a-bb04-98df1db2411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = datetime.utcnow().isoformat()\n",
    "trainer.train()\n",
    "train_end = datetime.utcnow().isoformat()\n",
    "\n",
    "history = get_history(trainer)\n",
    "save_json({\"train_start\": train_start, \"train_end\": train_end, \"history\": history}, OUT_DIR / \"trainer_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e21efe-d030-4ab1-9240-871a29243490",
   "metadata": {},
   "source": [
    "# 9. EVALUATION & METRICS (DEV & TEST)\n",
    "Evaluate on both dev and test sets and save metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84626128-1750-429d-b49b-6fb3c7e81638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_metrics = trainer.evaluate(eval_dataset=dev_tok)\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_tok)\n",
    "\n",
    "print(\"DEV:\", dev_metrics)\n",
    "print(\"TEST:\", test_metrics)\n",
    "\n",
    "save_json({\"dev\": dev_metrics, \"test\": test_metrics}, OUT_DIR / \"metrics.json\")\n",
    "\n",
    "trainer.save_model(str(OUT_DIR / \"model\"))\n",
    "tokenizer.save_pretrained(str(OUT_DIR / \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f93a0-c649-446f-99d2-d47e282edce5",
   "metadata": {},
   "source": [
    "# 10. DETAILED TEST REPORT & CONFUSION MATRIX\n",
    "Save a full classification report and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a193f27-7777-4c50-9498-7f43259444f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = trainer.predict(test_tok)\n",
    "logits = pred_output.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(labels, preds, target_names=[label_map[i] for i in [0,1,2]], zero_division=0, output_dict=True)\n",
    "save_json(clf_report, OUT_DIR / \"classification_report.json\")\n",
    "print(\"Classification report (summary saved).\")\n",
    "\n",
    "# Confusion matrix & plot\n",
    "cm = confusion_matrix(labels, preds, labels=[0,1,2])\n",
    "cm_display = {\n",
    "    \"matrix\": cm.tolist(),\n",
    "    \"labels\": [label_map[i] for i in [0,1,2]],\n",
    "}\n",
    "save_json(cm_display, OUT_DIR / \"confusion_matrix.json\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[label_map[i] for i in [0,1,2]], yticklabels=[label_map[i] for i in [0,1,2]])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - ANLI Test\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"confusion_matrix.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360947d-45de-4e83-bbaf-f225526ce7df",
   "metadata": {},
   "source": [
    "# 11. TRAINING CURVES (LOSS / METRICS) PLOT\n",
    "We plot training loss and evaluation macro-F1 to inspect learning dynamics and possible overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad533c1-fb21-4b7a-8bd3-fb495e0a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.state.log_history\n",
    "\n",
    "# Converting to Arrays\n",
    "steps = [h.get(\"step\") for h in history if \"step\" in h]\n",
    "train_losses = [h[\"loss\"] for h in history if \"loss\" in h]\n",
    "eval_steps = [h[\"step\"] for h in history if any(k.startswith(\"eval_\") for k in h)]\n",
    "eval_f1 = [h.get(\"eval_macro_f1\") for h in history if \"eval_macro_f1\" in h]\n",
    "eval_acc = [h.get(\"eval_accuracy\") for h in history if \"eval_accuracy\" in h]\n",
    "eval_loss = [h.get(\"eval_loss\") for h in history if \"eval_loss\" in h]\n",
    "\n",
    "# Loss plot\n",
    "plt.figure()\n",
    "plt.plot(steps[:len(train_losses)], train_losses, label=\"train_loss\")\n",
    "if eval_steps:\n",
    "    plt.scatter(eval_steps, eval_loss, label=\"eval_loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss during training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"training_loss.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Macro F1 plot\n",
    "if eval_f1:\n",
    "    plt.figure()\n",
    "    plt.plot(eval_steps[:len(eval_f1)], eval_f1, marker=\"o\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"Eval Macro F1 during training\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"eval_macro_f1.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9507a-bf11-465f-8ab6-8613575ffd0b",
   "metadata": {},
   "source": [
    "# 12. SAVE PARAMETERS & REPRODUCIBILITY NOTES\n",
    "We save a short JSON describing the hyperparameters, model, and run notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa97021-b871-4ae5-a3e9-044b3299c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save environment & reproducibility notes\n",
    "repro = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_acc\": GRAD_ACC,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"seed\": SEED,\n",
    "    \"use_all_rounds\": USE_ALL_ROUNDS,\n",
    "    \"device\": DEVICE,\n",
    "    \"notes\": \"Check trainer_history.json for stepwise logs.\"\n",
    "}\n",
    "save_json(repro, OUT_DIR / \"reproducibility.json\")\n",
    "print(f\"Saved artifacts to {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed244f29-63f7-4098-8a05-eeb2bfe801bb",
   "metadata": {},
   "source": [
    "## 13. Final notes & next steps\n",
    "\n",
    "- If memory errors occur: lower BATCH_SIZE or use gradient checkpointing / smaller model.\n",
    "- Consider alternatives to simple upsampling:\n",
    "  - Class-weighted loss (e.g., `weight` in CrossEntropyLoss)\n",
    "  - Efficient Augmentation (paraphrasing)\n",
    "- For best performance on ANLI, consider:\n",
    "  - Training on all rounds (we provided a toggle)\n",
    "  - Trying DeBERTa v3-large / -xlarge models\n",
    "  - Adversarial training, ensembling, or multi-task learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64064111-afdb-4824-afa4-043d503a922b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
