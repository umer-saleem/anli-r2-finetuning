{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8113ca-cf81-436d-9223-6cd769ff9d8d",
   "metadata": {},
   "source": [
    "# HIGH-PERFORMANCE ANLI FINETUNING (GPU ENABLED)\n",
    "**Goals**\n",
    "- End-to-end ML pipeline: EDA → Preprocessing → Training → Evaluation → Save & Deploy.\n",
    "- Save metrics, plots, confusion matrix, model, and tokenizer.\n",
    "- Provide GitHub & Docker deployment instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37035632-8039-4ac8-939c-7438ad8d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback,\n",
    "    set_seed\n",
    "\n",
    "# Output Directories\n",
    "OUT_DIR = Path(\"./anli_best_results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=4, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cee28-2a85-41be-9fa2-0dc28366dbd7",
   "metadata": {},
   "source": [
    "# 0. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913d0b3c-90c1-43ca-8832-9cb93f964e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      9\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m---> 10\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "USE_ALL_ROUNDS = True \n",
    "MODEL_NAME = \"roberta-large\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8              \n",
    "GRAD_ACC = 4                \n",
    "LR = 1e-5\n",
    "NUM_EPOCHS = 3\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983d5de-3538-4e35-8a79-558a323e21bf",
   "metadata": {},
   "source": [
    "# 1. REPRODUCIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bbbb7-74ad-4a5b-b19c-6407f0c82bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31226-4ab9-4168-ae1b-654883d2d047",
   "metadata": {},
   "source": [
    "# 2. LOAD DATA & SIMPLE EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e685aa4-9411-4065-b2e2-2be85a753a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell: Load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/anli\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_ALL_ROUNDS:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>> Using ALL rounds: R1 + R2 + R3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"facebook/anli\")\n",
    "\n",
    "if USE_ALL_ROUNDS:\n",
    "    train = concatenate_datasets([dataset[\"train_r1\"], dataset[\"train_r2\"], dataset[\"train_r3\"]])\n",
    "    dev   = concatenate_datasets([dataset[\"dev_r1\"],   dataset[\"dev_r2\"],   dataset[\"dev_r3\"]])\n",
    "    test  = concatenate_datasets([dataset[\"test_r1\"],  dataset[\"test_r2\"],  dataset[\"test_r3\"]])\n",
    "else:\n",
    "    train = dataset[\"train_r2\"]\n",
    "    dev   = dataset[\"dev_r2\"]\n",
    "    test  = dataset[\"test_r2\"]\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5029dab-64e5-4db2-bfeb-bd706a0486e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_stats(ds, name):\n",
    "    labels = np.array(ds[\"label\"])\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"{name} size: {len(ds)}\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  {label_map[u]:12s}: {c} ({c/len(ds):.2%})\")\n",
    "\n",
    "dataset_stats(train, \"Train\")\n",
    "dataset_stats(dev, \"Dev\")\n",
    "dataset_stats(test, \"Test\")\n",
    "\n",
    "eda_summary = {\n",
    "    \"train_size\": len(train),\n",
    "    \"dev_size\": len(dev),\n",
    "    \"test_size\": len(test),\n",
    "    \"train_class_counts\": dict(zip(*np.unique(np.array(train[\"label\"]), return_counts=True))),\n",
    "}\n",
    "save_json(eda_summary, OUT_DIR / \"eda_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38626cd1-15cf-4d7d-a3d0-b49698444acb",
   "metadata": {},
   "source": [
    "# 3. TOKENIZER & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5424fa-1046-4b48-8818-c897b88e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"premise\"], batch[\"hypothesis\"],\n",
    "                     truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "train_tok = train.map(preprocess, batched=True, remove_columns=train.column_names)\n",
    "dev_tok   = dev.map(preprocess,   batched=True, remove_columns=dev.column_names)\n",
    "test_tok  = test.map(preprocess,  batched=True, remove_columns=test.column_names)\n",
    "\n",
    "train_tok = train_tok.add_column(\"labels\", train[\"label\"])\n",
    "dev_tok   = dev_tok.add_column(\"labels\", dev[\"label\"])\n",
    "test_tok  = test_tok.add_column(\"labels\", test[\"label\"])\n",
    "\n",
    "train_tok.set_format(type=\"torch\")\n",
    "dev_tok.set_format(type=\"torch\")\n",
    "test_tok.set_format(type=\"torch\")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0dd46-70dc-4a74-84bd-c8f489759f4d",
   "metadata": {},
   "source": [
    "# 4. MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52291-1012-433a-a6a3-80f744c1c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93add1a2-0a09-4f3b-8101-230ff9fa913c",
   "metadata": {},
   "source": [
    "# 5. METRICS & EVAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b41e8-fcfb-4e23-8d1f-88c858857d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, labels=[0,1,2], average=None, zero_division=0)\n",
    "    macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"f1_entailment\": float(f1[0]),\n",
    "        \"f1_neutral\": float(f1[1]),\n",
    "        \"f1_contradiction\": float(f1[2]),\n",
    "    }\n",
    "\n",
    "def get_history(trainer):\n",
    "    history = trainer.state.log_history\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05fd64-639f-41d7-9439-0fdc735a9105",
   "metadata": {},
   "source": [
    "# 6. TRAINING ARGUMENTS & CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc5c93-3715-4763-a2e6-09a4b79fcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True if DEVICE == \"cuda\" else False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "earlystop = EarlyStoppingCallback(early_stopping_patience=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099fce-ee2e-4561-9c8e-dea4c815e481",
   "metadata": {},
   "source": [
    "# 7. TRAINER SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106c242-b550-4e03-b356-f7549ffdb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[earlystop],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cf669-ebc8-463a-8ba9-e4c744815133",
   "metadata": {},
   "source": [
    "# 8. TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6eeaf9-de9e-464a-bb04-98df1db2411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Train and capture history\n",
    "train_start = datetime.utcnow().isoformat()\n",
    "trainer.train()\n",
    "train_end = datetime.utcnow().isoformat()\n",
    "\n",
    "# Save trainer log history\n",
    "history = get_history(trainer)\n",
    "save_json({\"train_start\": train_start, \"train_end\": train_end, \"history\": history}, OUT_DIR / \"trainer_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e21efe-d030-4ab1-9240-871a29243490",
   "metadata": {},
   "source": [
    "# 9. EVALUATION & METRICS (DEV & TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84626128-1750-429d-b49b-6fb3c7e81638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Evaluate on dev & test\n",
    "dev_metrics = trainer.evaluate(eval_dataset=dev_tok)\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_tok)\n",
    "\n",
    "print(\"DEV:\", dev_metrics)\n",
    "print(\"TEST:\", test_metrics)\n",
    "\n",
    "save_json({\"dev\": dev_metrics, \"test\": test_metrics}, OUT_DIR / \"metrics.json\")\n",
    "\n",
    "# Save best model + tokenizer\n",
    "trainer.save_model(str(OUT_DIR / \"model\"))\n",
    "tokenizer.save_pretrained(str(OUT_DIR / \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f93a0-c649-446f-99d2-d47e282edce5",
   "metadata": {},
   "source": [
    "# 10. DETAILED TEST REPORT & CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a193f27-7777-4c50-9498-7f43259444f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Predictions for test set and confusion matrix\n",
    "pred_output = trainer.predict(test_tok)\n",
    "logits = pred_output.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(labels, preds, target_names=[label_map[i] for i in [0,1,2]], zero_division=0, output_dict=True)\n",
    "save_json(clf_report, OUT_DIR / \"classification_report.json\")\n",
    "print(\"Classification report (summary saved).\")\n",
    "\n",
    "# Confusion matrix & plot\n",
    "cm = confusion_matrix(labels, preds, labels=[0,1,2])\n",
    "cm_display = {\n",
    "    \"matrix\": cm.tolist(),\n",
    "    \"labels\": [label_map[i] for i in [0,1,2]],\n",
    "}\n",
    "save_json(cm_display, OUT_DIR / \"confusion_matrix.json\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[label_map[i] for i in [0,1,2]], yticklabels=[label_map[i] for i in [0,1,2]])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - ANLI Test\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"confusion_matrix.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360947d-45de-4e83-bbaf-f225526ce7df",
   "metadata": {},
   "source": [
    "# 11. TRAINING CURVES (LOSS / METRICS) PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad533c1-fb21-4b7a-8bd3-fb495e0a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Plot training & eval metrics extracted from trainer history\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Convert to arrays\n",
    "steps = [h.get(\"step\") for h in history if \"step\" in h]\n",
    "train_losses = [h[\"loss\"] for h in history if \"loss\" in h]\n",
    "eval_steps = [h[\"step\"] for h in history if any(k.startswith(\"eval_\") for k in h)]\n",
    "eval_f1 = [h.get(\"eval_macro_f1\") for h in history if \"eval_macro_f1\" in h]\n",
    "eval_acc = [h.get(\"eval_accuracy\") for h in history if \"eval_accuracy\" in h]\n",
    "eval_loss = [h.get(\"eval_loss\") for h in history if \"eval_loss\" in h]\n",
    "\n",
    "# Loss plot\n",
    "plt.figure()\n",
    "plt.plot(steps[:len(train_losses)], train_losses, label=\"train_loss\")\n",
    "if eval_steps:\n",
    "    plt.scatter(eval_steps, eval_loss, label=\"eval_loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss during training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"training_loss.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# Macro F1 plot\n",
    "if eval_f1:\n",
    "    plt.figure()\n",
    "    plt.plot(eval_steps[:len(eval_f1)], eval_f1, marker=\"o\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"Eval Macro F1 during training\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"eval_macro_f1.png\", dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9507a-bf11-465f-8ab6-8613575ffd0b",
   "metadata": {},
   "source": [
    "# 12. SAVE ARTIFACTS & REPRODUCIBILITY NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa97021-b871-4ae5-a3e9-044b3299c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Save environment & reproducibility notes\n",
    "repro = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_acc\": GRAD_ACC,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"seed\": SEED,\n",
    "    \"use_all_rounds\": USE_ALL_ROUNDS,\n",
    "    \"device\": DEVICE,\n",
    "    \"notes\": \"Check trainer_history.json for stepwise logs.\"\n",
    "}\n",
    "save_json(repro, OUT_DIR / \"reproducibility.json\")\n",
    "print(f\"Saved artifacts to {OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
