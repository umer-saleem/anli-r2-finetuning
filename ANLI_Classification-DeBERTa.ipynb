{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8113ca-cf81-436d-9223-6cd769ff9d8d",
   "metadata": {},
   "source": [
    "# ANLI Classification using DeBERTa (GPU ENABLED)\n",
    "**Goals**\n",
    "- End-to-end ML pipeline: EDA → Preprocessing → Training → Evaluation → Save & Deploy.\n",
    "- Save metrics, plots, confusion matrix, model, and tokenizer.\n",
    "- Provide GitHub & Docker deployment instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37035632-8039-4ac8-939c-7438ad8d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Imports & Utilities\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    classification_report,\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback, set_seed\n",
    "\n",
    "\n",
    "# Output directories\n",
    "OUT_DIR = Path(\"/content/anli_best_results_deberta_r2\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=4, default=str)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cee28-2a85-41be-9fa2-0dc28366dbd7",
   "metadata": {},
   "source": [
    "# 0. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d0b3c-90c1-43ca-8832-9cb93f964e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALL_ROUNDS = False \n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "MAX_LENGTH = 384  # increased from 256 -> 384\n",
    "BATCH_SIZE = 8  # per device (kept same as your original)\n",
    "GRAD_ACC = 4  # gradient accumulation\n",
    "LR = 1e-5\n",
    "NUM_EPOCHS = 3\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983d5de-3538-4e35-8a79-558a323e21bf",
   "metadata": {},
   "source": [
    "# 1. REPRODUCIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bbbb7-74ad-4a5b-b19c-6407f0c82bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31226-4ab9-4168-ae1b-654883d2d047",
   "metadata": {},
   "source": [
    "# 2. LOAD DATA & SIMPLE EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e685aa4-9411-4065-b2e2-2be85a753a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/anli\")\n",
    "\n",
    "train = dataset[\"train_r2\"]\n",
    "dev   = dataset[\"dev_r2\"]\n",
    "test  = dataset[\"test_r2\"]\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "\n",
    "# Quick EDA function\n",
    "def dataset_stats(ds, name):\n",
    "    labels = np.array(ds[\"label\"])\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"{name} size: {len(ds)}\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  {label_map[int(u)]:12s}: {c} ({c/len(ds):.2%})\")\n",
    "\n",
    "dataset_stats(train, \"Train\")\n",
    "dataset_stats(dev, \"Dev\")\n",
    "dataset_stats(test, \"Test\")\n",
    "\n",
    "# Save a quick EDA file\n",
    "unique, counts = np.unique(np.array(train[\"label\"]), return_counts=True)\n",
    "train_class_counts = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "\n",
    "eda_summary = {\"train_size\": len(train), \"dev_size\": len(dev), \"test_size\": len(test),\"train_class_counts\": train_class_counts}\n",
    "\n",
    "save_json(eda_summary, OUT_DIR / \"eda_summary.json\")\n",
    "print(f\"Saved EDA summary to {OUT_DIR / 'eda_summary.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5029dab-64e5-4db2-bfeb-bd706a0486e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BALANCE THE TRAINING SET (UPSAMPLING MINORITY CLASSES)\n",
    "\n",
    "print(\"Original class counts:\", Counter(train[\"label\"]))\n",
    "label_counts = Counter(train[\"label\"])\n",
    "max_count = max(label_counts.values())\n",
    "\n",
    "balanced_splits = []\n",
    "for label in [0, 1, 2]:\n",
    "    subset = train.filter(lambda x: x[\"label\"] == int(label))\n",
    "    subset_dict = subset.to_dict()\n",
    "    n = len(subset)\n",
    "    if n == 0:\n",
    "        continue\n",
    "    # How many repeats to exceed or reach max_count\n",
    "    repeat_factor = max_count // n + (1 if max_count % n != 0 else 0)\n",
    "    # Repeat each field in subset_dict\n",
    "    repeated = {k: (v * repeat_factor)[: max_count] for k, v in subset_dict.items()}\n",
    "    upsampled = Dataset.from_dict(repeated)\n",
    "    upsampled = upsampled.shuffle(seed=SEED)\n",
    "    balanced_splits.append(upsampled)\n",
    "\n",
    "# Concatenate balanced splits and shuffle\n",
    "train_balanced = concatenate_datasets(balanced_splits).shuffle(seed=SEED)\n",
    "print(\"Balanced class counts:\", Counter(train_balanced[\"label\"]))\n",
    "print(\"Balanced train size:\", len(train_balanced))\n",
    "\n",
    "# Replace train with balanced version for the rest of pipeline\n",
    "train = train_balanced\n",
    "\n",
    "# Save new EDA after balancing\n",
    "unique, counts = np.unique(np.array(train[\"label\"]), return_counts=True)\n",
    "train_class_counts_balanced = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "eda_summary[\"train_class_counts_balanced\"] = train_class_counts_balanced\n",
    "eda_summary[\"train_balanced_size\"] = len(train)\n",
    "save_json(eda_summary, OUT_DIR / \"eda_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38626cd1-15cf-4d7d-a3d0-b49698444acb",
   "metadata": {},
   "source": [
    "# 3. TOKENIZER & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5424fa-1046-4b48-8818-c897b88e46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"premise\"], batch[\"hypothesis\"],\n",
    "                     truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "train_tok = train.map(preprocess, batched=True, remove_columns=train.column_names)\n",
    "dev_tok   = dev.map(preprocess,   batched=True, remove_columns=dev.column_names)\n",
    "test_tok  = test.map(preprocess,  batched=True, remove_columns=test.column_names)\n",
    "\n",
    "# add labels back and set to torch format\n",
    "train_tok = train_tok.add_column(\"labels\", train[\"label\"])\n",
    "dev_tok   = dev_tok.add_column(\"labels\", dev[\"label\"])\n",
    "test_tok  = test_tok.add_column(\"labels\", test[\"label\"])\n",
    "\n",
    "train_tok.set_format(type=\"torch\")\n",
    "dev_tok.set_format(type=\"torch\")\n",
    "test_tok.set_format(type=\"torch\")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "print(\"Tokenization complete. Example tokenized fields:\", list(train_tok.features.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0dd46-70dc-4a74-84bd-c8f489759f4d",
   "metadata": {},
   "source": [
    "# 4. MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52291-1012-433a-a6a3-80f744c1c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "model.to(DEVICE)\n",
    "print(f\"Loaded model {MODEL_NAME} and moved to {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93add1a2-0a09-4f3b-8101-230ff9fa913c",
   "metadata": {},
   "source": [
    "# 5. METRICS & EVAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b41e8-fcfb-4e23-8d1f-88c858857d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, labels=[0,1,2], average=None, zero_division=0)\n",
    "    macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"f1_entailment\": float(f1[0]),\n",
    "        \"f1_neutral\": float(f1[1]),\n",
    "        \"f1_contradiction\": float(f1[2]),\n",
    "    }\n",
    "\n",
    "def get_history(trainer):\n",
    "    history = trainer.state.log_history\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05fd64-639f-41d7-9439-0fdc735a9105",
   "metadata": {},
   "source": [
    "# 6. TRAINING ARGUMENTS & CALLBACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc5c93-3715-4763-a2e6-09a4b79fcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True if DEVICE == \"cuda\" else False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "earlystop = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "print(\"TrainingArguments prepared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099fce-ee2e-4561-9c8e-dea4c815e481",
   "metadata": {},
   "source": [
    "# 7. TRAINER SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106c242-b550-4e03-b356-f7549ffdb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[earlystop],\n",
    ")\n",
    "print(\"Trainer initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cf669-ebc8-463a-8ba9-e4c744815133",
   "metadata": {},
   "source": [
    "# 8. TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6eeaf9-de9e-464a-bb04-98df1db2411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = datetime.utcnow().isoformat()\n",
    "train_result = trainer.train()\n",
    "train_end = datetime.utcnow().isoformat()\n",
    "\n",
    "history = get_history(trainer)\n",
    "save_json({\"train_start\": train_start, \"train_end\": train_end, \"history\": history}, OUT_DIR / \"trainer_history.json\")\n",
    "\n",
    "trainer.save_model(str(OUT_DIR / \"best_model\"))\n",
    "tokenizer.save_pretrained(str(OUT_DIR / \"best_model\"))\n",
    "print(f\"Training completed. Model and tokenizer saved to {OUT_DIR / 'best_model'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e21efe-d030-4ab1-9240-871a29243490",
   "metadata": {},
   "source": [
    "# 9. EVALUATION & METRICS (DEV & TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84626128-1750-429d-b49b-6fb3c7e81638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_metrics = trainer.evaluate(eval_dataset=dev_tok)\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_tok)\n",
    "\n",
    "print(\"DEV:\", dev_metrics)\n",
    "print(\"TEST:\", test_metrics)\n",
    "\n",
    "save_json({\"dev\": dev_metrics, \"test\": test_metrics}, OUT_DIR / \"metrics.json\")\n",
    "print(f\"Saved dev/test metrics to {OUT_DIR / 'metrics.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f93a0-c649-446f-99d2-d47e282edce5",
   "metadata": {},
   "source": [
    "# 10. DETAILED TEST REPORT & CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a193f27-7777-4c50-9498-7f43259444f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = trainer.predict(test_tok)\n",
    "logits = pred_output.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "# Classification Report\n",
    "clf_report = classification_report(labels, preds, target_names=[label_map[i] for i in [0,1,2]], zero_division=0, output_dict=True)\n",
    "save_json(clf_report, OUT_DIR / \"classification_report.json\")\n",
    "print(\"Classification report saved.\")\n",
    "\n",
    "# Confusion matrix & plot\n",
    "cm = confusion_matrix(labels, preds, labels=[0,1,2])\n",
    "cm_display = {\n",
    "    \"matrix\": cm.tolist(),\n",
    "    \"labels\": [label_map[i] for i in [0,1,2]],\n",
    "}\n",
    "save_json(cm_display, OUT_DIR / \"confusion_matrix.json\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[label_map[i] for i in [0,1,2]], yticklabels=[label_map[i] for i in [0,1,2]])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - ANLI R2 Test\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"confusion_matrix.png\", dpi=200)\n",
    "plt.show()\n",
    "print(f\"Saved confusion matrix image to {OUT_DIR / 'confusion_matrix.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360947d-45de-4e83-bbaf-f225526ce7df",
   "metadata": {},
   "source": [
    "# 11. TRAINING CURVES (LOSS / METRICS) PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad533c1-fb21-4b7a-8bd3-fb495e0a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.state.log_history\n",
    "\n",
    "steps = [h.get(\"step\") for h in history if \"step\" in h and \"loss\" in h]\n",
    "train_losses = [h[\"loss\"] for h in history if \"loss\" in h]\n",
    "\n",
    "eval_steps = [h[\"step\"] for h in history if \"step\" in h and any(k.startswith(\"eval_\") for k in h)]\n",
    "eval_f1 = [h.get(\"eval_macro_f1\") for h in history if \"eval_macro_f1\" in h]\n",
    "eval_acc = [h.get(\"eval_accuracy\") for h in history if \"eval_accuracy\" in h]\n",
    "eval_loss = [h.get(\"eval_loss\") for h in history if \"eval_loss\" in h]\n",
    "\n",
    "# Plot training loss\n",
    "if len(steps) and len(train_losses):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(steps[:len(train_losses)], train_losses, marker='o')\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"train_loss.png\", dpi=200)\n",
    "    plt.show()\n",
    "    print(f\"Saved training loss to {OUT_DIR / 'train_loss.png'}\")\n",
    "\n",
    "# Plot eval metrics\n",
    "if len(eval_steps):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    if any(val is not None for val in eval_f1):\n",
    "        plt.plot(eval_steps, eval_f1, label=\"Eval Macro F1\", marker='o')\n",
    "    if any(val is not None for val in eval_acc):\n",
    "        plt.plot(eval_steps, eval_acc, label=\"Eval Accuracy\", marker='o')\n",
    "    if any(val is not None for val in eval_loss):\n",
    "        plt.plot(eval_steps, eval_loss, label=\"Eval Loss\", marker='o')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.title(\"Evaluation Metrics Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"eval_metrics.png\", dpi=200)\n",
    "    plt.show()\n",
    "    print(f\"Saved eval metrics to {OUT_DIR / 'eval_metrics.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9507a-bf11-465f-8ab6-8613575ffd0b",
   "metadata": {},
   "source": [
    "# 12. SAVE PARAMETERS & REPRODUCIBILITY NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa97021-b871-4ae5-a3e9-044b3299c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repro = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_acc\": GRAD_ACC,\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"seed\": SEED,\n",
    "    \"use_all_rounds\": USE_ALL_ROUNDS,\n",
    "    \"device\": DEVICE,\n",
    "    \"notes\": \"Check trainer_history.json for stepwise logs.\"\n",
    "}\n",
    "save_json(repro, OUT_DIR / \"reproducibility.json\")\n",
    "print(f\"Saved artifacts to {OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
